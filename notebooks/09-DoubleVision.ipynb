{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double box version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.externals import joblib\n",
    "from keras.models import model_from_json\n",
    "from moviepy.editor import VideoFileClip\n",
    "from scipy.ndimage.measurements import label\n",
    "from skimage.feature import local_binary_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Metaparameters\n",
    "colorspace= 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient= 12\n",
    "pix_per_cell= 4\n",
    "cell_per_block= 1 #2\n",
    "hog_channel= 'ALL' # Can be 0, 1, 2, or \"ALL\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Customized re-implementation of HOG, based on OpenCV and scikit-image versions\n",
    "def histoGrad(img, binCount=12, cellSize=4):\n",
    "    gx,gy= cv2.Sobel(img,cv2.CV_32F,1,0), cv2.Sobel(img,cv2.CV_32F,0,1) # gradients in x and y direction\n",
    "    magnitude,angle= cv2.cartToPolar(gx,gy)\n",
    "    bins= np.int32(binCount*angle*(1-1e-7)/(2*np.pi)) #scale back angle to avoid overbinning\n",
    "    binCells,magCells= [],[]\n",
    "    cx= cy= cellSize\n",
    "    for i in range(0,int(img.shape[0]/cy)): # sort into bins\n",
    "        for j in range(0,int(img.shape[1]/cx)):\n",
    "            binCells.append(bins[i*cy:i*cy+cy, j*cx:j*cx+cx])\n",
    "            magCells.append(magnitude[i*cy:i*cy+cy, j*cx:j*cx+cx])\n",
    "    hist= np.hstack([np.bincount(i.ravel(), j.ravel(), binCount) for i,j in zip(binCells, magCells)])\n",
    "    return np.sqrt(hist/(hist.sum()+1e-8)) # apply Hellinger kernel (L1-normalize and sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to return HOG features\n",
    "def get_hog_features(img, orient, pix_per_cell):\n",
    "    features= histoGrad( img, orient, pix_per_cell )\n",
    "    return features.reshape((orient*pix_per_cell*pix_per_cell,))    #192\n",
    "\n",
    "# Function to return color histogram features  \n",
    "def get_color_hist(img, nbins=32, bins_range=(0, 32)):\n",
    "    return np.histogram(img, bins=nbins, range=bins_range)\n",
    "\n",
    "# Function to return spatial binning of color features\n",
    "def get_spatial(img, size=(8, 8)):\n",
    "    return np.ravel(cv2.resize(img, size, interpolation=3) )/255.\n",
    "\n",
    "# Function to return texture features\n",
    "def get_texture_features(img, quantization=8, radius=3):\n",
    "    lbp= np.ravel(local_binary_pattern(img, quantization, radius, method='uniform'))\n",
    "    return np.mean(lbp.reshape(-1, 4), axis=1) # average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dropout_1 (Dropout)              (None, 624)           0           dropout_input_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 255)           159375      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 255)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 127)           32512       dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 127)           0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             128         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 192,015\n",
      "Trainable params: 192,015\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load binary classifier model\n",
    "\n",
    "model= None\n",
    "with open('model.json', 'r') as jfile:\n",
    "    model = model_from_json(json.loads(jfile.read()))\n",
    "model.compile(\"adam\", \"binary_crossentropy\")\n",
    "#weights_file = args.model.replace('json', 'h5')\n",
    "model.load_weights('model.h5')\n",
    "model.summary()\n",
    "X_scaler= joblib.load('scalar.dat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getFeaturesFromColorPatch(patch, orient=12, pix_per_cell=4, cell_per_block=1):\n",
    "    global firstTime\n",
    "    hog_features = []\n",
    "    feature_patch= cv2.cvtColor(patch, cv2.COLOR_RGB2YCrCb)\n",
    "    for channel in range(feature_patch.shape[2]):\n",
    "        #hog_features.append(get_hog_features(feature_patch[:,:,channel], \n",
    "        #                    orient, pix_per_cell, cell_per_block, \n",
    "        #                    vis=False, feature_vec=True))\n",
    "        \n",
    "        # Compute gradient AND small color patch (note: color scaling)\n",
    "        hog_features.append(np.concatenate((get_hog_features(feature_patch[:,:,channel], orient, pix_per_cell), \n",
    "            np.ravel(cv2.resize(feature_patch[:,:,channel], (4,4), interpolation=3) )/255. ))) #CV_INTER_AREA\n",
    "        #hog_features.append(np.concatenate((get_hog_features(feature_patch[:,:,channel], \n",
    "        #                    orient, pix_per_cell, cell_per_block, \n",
    "        #                    vis=False, feature_vec=True), get_lpb_features(feature_patch[:,:,channel]))))\n",
    "        #hog_features.append(np.zeros(432))   # <--- short out\n",
    "    return np.ravel(hog_features)\n",
    "\n",
    "def scalePatch(patch):\n",
    "    global X_scaler # use previously defined scaler\n",
    "    X= patch.astype(np.float64).reshape((1, -1))                       \n",
    "    # Apply the scaler to X\n",
    "    return np.array(X_scaler.transform(X))\n",
    "\n",
    "#_______________________________________________________________________________\n",
    "# Circular queuing class made with Numpy\n",
    "\n",
    "class NQ():\n",
    "    def __init__(self,qlen,shape,dtype=np.uint8):\n",
    "        self.data= np.empty(shape=shape,dtype=dtype)\n",
    "        self.qlen= qlen\n",
    "    def put(self,a):\n",
    "        self.data= np.append(self.data,a, axis=0)\n",
    "        if len(self.data)>self.qlen: self.drop()\n",
    "    def peek(self):\n",
    "        return self.data[len(self.data)-1]\n",
    "    def drop(self):\n",
    "        self.data= np.delete(self.data, 0, axis=0)\n",
    "    def getAvg(self):\n",
    "        newList= np.copy(self.data[0])\n",
    "        newList[:]= np.sum(self.data[:], axis=0)/float(len(self.data))\n",
    "        return newList\n",
    "\n",
    "def renderRig(img, cx,cy, width,height, clr, strokeWeight):\n",
    "    pts=np.array([[0.75,0.1275], [0.4866666667,0.085], [0.286666667,0.1025], [0,0.3875], [0,0.645], [0.47,0.9275], [1,0.8825], [1,0.38]])\n",
    "    pts[:,0]= pts[:,0]*width  + cx-width //2\n",
    "    pts[:,1]= pts[:,1]*height + cy-height//2\n",
    "    cv2.polylines(img, [pts.astype(np.int32)], True, clr, strokeWeight)\n",
    "\n",
    "x= np.linspace(0, 14, 15)\n",
    "y=np.array([766,780,796,814,834,857,882,910,941,976,1014,1057,1103,1155,1214])\n",
    "coefs1= np.polyfit(x,y,3)\n",
    "y=np.array([411,410,410,409,409,408,407,406,406,405,403,402,401,398,397])\n",
    "coefs2= np.polyfit(x,y,2)\n",
    "y=np.array([14,16,18,20,23,25,28,31,35,38,43,46,52,59,66])\n",
    "coefs3= np.polyfit(x,y,2)\n",
    "def transformIdxToScreen(idxFloat):\n",
    "    global coefs1, coefs2, coefs3\n",
    "    f1= np.poly1d(coefs1)\n",
    "    f2= np.poly1d(coefs2)\n",
    "    f3= np.poly1d(coefs3)\n",
    "    return f1(idxFloat), f2(idxFloat), f3(idxFloat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "    return heatmap\n",
    "    \n",
    "def apply_threshold(heatmap, threshold):\n",
    "    heatmap[heatmap <= threshold] = 0 # Zero out pixels below the threshold\n",
    "    return heatmap\n",
    "\n",
    "def draw_labeled_bboxes(img, labels):\n",
    "    global frameCount\n",
    "    # Iterate through all detected cars\n",
    "#    if labels[1]>2: print(frameCount,labels[1])\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero= (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy= np.array(nonzero[0])\n",
    "        nonzerox= np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox= ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], (0,150,255), 5)\n",
    "\n",
    "box_list= [[], []]  # define two box lists, for adjacent lanes\n",
    "#box_list= NQ(6, (0,2), np.array)\n",
    "\n",
    "def addBoxToList(idx, x,y,s): #add a screen box for each of the positive cells\n",
    "    global box_list\n",
    "    box_list[idx].append( ((x, y), (x+s+5, y+s+5)) ) # make the cells slightly larger so they overlap\n",
    "\n",
    "def drawData(img):\n",
    "    global box_list, frameCount\n",
    "    for i in range(0,2):\n",
    "        heat= np.zeros_like(img[:,:,0]).astype(np.float)\n",
    "        # Add heat to each box in box list\n",
    "        heat= add_heat(heat,box_list[i])\n",
    "\n",
    "        # Apply threshold to help remove false positives\n",
    "        heat= apply_threshold(heat,2)\n",
    "\n",
    "        # Find final boxes from heatmap using label function\n",
    "        labels= label(heat)  #label(heat)\n",
    "        #if frameCount==246:\n",
    "        #    plt.figure(figsize=(16,9))\n",
    "        #    plt.imshow(np.clip(heat, 0, 255), cmap='hot')\n",
    "        #    plt.show()\n",
    "        draw_labeled_bboxes(img, labels)\n",
    "        \n",
    "        # Clean up the list\n",
    "        boxQLEN= 20\n",
    "        if len(box_list[i])>boxQLEN: # keep the last n boxes\n",
    "            top= len(box_list[i])-boxQLEN\n",
    "            for j in range(0,top): # circular queue compatible with code from lecture notes \n",
    "                del box_list[i][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video...\n",
      "[MoviePy] >>>> Building video output_images/project_video.mp4\n",
      "[MoviePy] Writing video output_images/project_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 1260/1261 [02:25<00:00,  8.94it/s]  0%|▏                                                                                                                          | 2/1261 [00:00<02:15,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_images/project_video.mp4 \n",
      "\n",
      "146.68 seconds to complete.  8.6 fps\n"
     ]
    }
   ],
   "source": [
    "# Batch version\n",
    "def resizePatch(img):\n",
    "    if len(img[0]) != 16:\n",
    "        if len(img[0])>16:\n",
    "            patch= cv2.resize(img, (16,16), interpolation=3) #CV_INTER_AREA\n",
    "        else:\n",
    "            patch= cv2.resize(img, (16,16), interpolation=2) #CV_INTER_CUBIC\n",
    "    else:\n",
    "        patch= img\n",
    "    return patch\n",
    "\n",
    "def preparePatch(p):\n",
    "    patch= resizePatch(p)\n",
    "    fpatch= getFeaturesFromColorPatch(patch)\n",
    "    spatch= scalePatch(fpatch)\n",
    "    return spatch\n",
    "\n",
    "diagScreen= None\n",
    "imgGlobal= None\n",
    "patchBatch= [[], [], [], [], []]  # patch, x, y, s, prediction\n",
    "predictionBatch= []\n",
    "\n",
    "def addPatch(p,x,y,s):\n",
    "    global patchBatch\n",
    "    patchBatch[0].append(p)\n",
    "    patchBatch[1].append(x)\n",
    "    patchBatch[2].append(y)\n",
    "    patchBatch[3].append(s)\n",
    "\n",
    "def processStripB(x,y,s):\n",
    "    global frameCount, imgGlobal\n",
    "    for i in range(4):\n",
    "        addPatch(imgGlobal[y:y+s, x:x+s, :],x,y,s)\n",
    "        y= y+s\n",
    "\n",
    "def processStripC(i):\n",
    "    global frameCount, imgGlobal\n",
    "    x,y,s= transformIdxToScreen(i)\n",
    "    for i in range(4):\n",
    "        addPatch(imgGlobal[y:y+s, x:x+s, :],x,y,s)\n",
    "        y= y+s\n",
    "\n",
    "# Predominant color assessment\n",
    "def dominantWhite(img): \n",
    "    return (img>90).sum() > (img<=90).sum()\n",
    "\n",
    "def locateCentroid(twoDimArray, prevCenterTuple):\n",
    "    global frameCount\n",
    "    mask= np.zeros((4,15),dtype=np.int32)\n",
    "    #mask= np.zeros_like(twoDimArray)               # <-----------------------------------------------------\n",
    "\n",
    "    cv2.ellipse(mask, (prevCenterTuple, (7,4),0), 1,-1)\n",
    "    #mask= np.array(mask, dtype=np.uint8)\n",
    "    masked= cv2.bitwise_and(mask, twoDimArray)\n",
    "    m= cv2.moments(np.array(masked),True)\n",
    "    return m['m10']/m['m00'], m['m01']/m['m00'] #return centroid float: x,y or col,row\n",
    "\n",
    "prevCentroid= (17.,2.)\n",
    "centroidQ= NQ(25, (0,2), np.int)\n",
    "for i in range(25): # bias average to offscreen \n",
    "    centroidQ.put([ np.array([1400,529]) ])\n",
    "lastDiagScreen= None\n",
    "rx= 0\n",
    "ry= 0\n",
    "rs= 0\n",
    "\n",
    "def processBatch(baseImage): # send the batch to the GPU for predictions\n",
    "    global frameCount, patchBatch, predictionBatch, diagScreen, prevCentroid, lastDiagScreen\n",
    "    global rx,ry,rs\n",
    "    prepBatch= []\n",
    "    for i in range( len(patchBatch[0]) ):\n",
    "        prepBatch.append( preparePatch(patchBatch[0][i]) )\n",
    "    predictionResults= model.predict( np.array(prepBatch).reshape((60,-1)) )\n",
    "    patchBatch[4]= np.array(predictionResults>0.9,dtype=np.uint8)\n",
    "    whiteCarFoundMaybe= False\n",
    "    for i in range( len(patchBatch[4]) ):\n",
    "        x= patchBatch[1][i]\n",
    "        y= patchBatch[2][i]\n",
    "        s= patchBatch[3][i]\n",
    "        if patchBatch[4][i]:\n",
    "            # RULE: if only rows 0 and 1 are positive, put both cells into lane 2 box list (index 0)\n",
    "            if i%4==0 and patchBatch[4][i] and patchBatch[4][i+1] and not patchBatch[4][i+2] and not patchBatch[4][i+3]:\n",
    "                addBoxToList(0, x,y,s)\n",
    "                addBoxToList(0, patchBatch[1][i+1],patchBatch[2][i+1],patchBatch[3][i+1])\n",
    "            # RULE: AND all cells go to lane 1 box list (index 1)\n",
    "            addBoxToList(1, x,y,s)\n",
    "\n",
    "            # Diagnostics: Display the positive cells\n",
    "            if dominantWhite(patchBatch[0][i]):\n",
    "                diagScreen[y:y+s-1, x:x+s-1, 0]= 63 # red\n",
    "                diagScreen[y:y+s-1, x:x+s-1, 1]= 31 # green\n",
    "                if whiteCarFoundMaybe==False:\n",
    "                    whiteCarFoundMaybe= True\n",
    "                    whiteCarLoc= i\n",
    "            else:\n",
    "                diagScreen[y:y+s-1, x:x+s-1, 1]= 63 # green\n",
    "\n",
    "        # Diagnostics: Display the negative cells\n",
    "        else:\n",
    "            diagScreen[y:y+s-1, x:x+s-1, 0]= 63   #157  Red\n",
    "        \n",
    "        # More training data\n",
    "        #if predictionResults[i]>0.9: # and predictionResults[i]>0.4:\n",
    "        #    mpimg.imsave('output_images/cells/'+str(frameCount)+'-'+str(x)+'-'+str(y)+'.PNG',resizePatch(patchBatch[0][i]))\n",
    "    #Search the neighbors of the white car in all directions\n",
    "#    if whiteCarFoundMaybe:\n",
    "#        refM= np.array(patchBatch[4].reshape((4,15),order='F'), dtype=np.uint8) # reference data matrix\n",
    "#        whiteCarLeft= 0 # assume maxes\n",
    "#        whiteCarRight= 14\n",
    "#        whiteCarTop= 0\n",
    "#        whiteCarBottom= 3\n",
    "#        if whiteCarLoc>4 and whiteCarLoc<56 and predictionBatch[i+4]>0.5 and predictionBatch[i-4]>0.5:\n",
    "\n",
    "        \n",
    "    if frameCount>=6000:  #  678 is when Black car arrives in project_video\n",
    "        for i in range(len(patchBatch[4])):\n",
    "            if dominantWhite(patchBatch[0][i]): patchBatch[4][i]=0 #mask white car for test purposes\n",
    "        refM= np.array(patchBatch[4].reshape((4,15),order='F'), dtype=np.int32) # reference data matrix\n",
    "        #refM.strides= (60,4)\n",
    "\n",
    "        newCentroid= locateCentroid(refM, prevCentroid)\n",
    "\n",
    "        newCentroid= (newCentroid[0], 1.5) #lock to middle of track\n",
    "\n",
    "        refX= np.array(patchBatch[1]).reshape((4,15),order='F')\n",
    "        refY= np.array(patchBatch[2]).reshape((4,15),order='F')\n",
    "        refS= np.array(patchBatch[3]).reshape((4,15),order='F')\n",
    "        x= refX[int(newCentroid[1]),round(newCentroid[0])]\n",
    "        y= refY[int(newCentroid[1]),int(newCentroid[0])]\n",
    "        #s= refS[int(newCentroid[1]),int(newCentroid[0])]\n",
    "        _,_,s= transformIdxToScreen(newCentroid[0])\n",
    "        x+= int( (newCentroid[0] - int(newCentroid[0]) ) * s) # add the fractional portion      s/2 +\n",
    "        y+= int(s/2 + (newCentroid[1] - int(newCentroid[1]) ) * s)\n",
    "        #diagScreen[y:y+10, x:x+10, 2]= 255  # small blue 10x10 square\n",
    "        centroidQ.put([ np.array([x,y]) ])\n",
    "\n",
    "        cv2.ellipse(diagScreen, (centroidQ.getAvg(), (s*6,s*4),0), (255,255,0), 2) # ellipse\n",
    "        #cx,cy= centroidQ.getAvg()\n",
    "        #stroke=3\n",
    "        #rigHeight=5\n",
    "        #rigWidth=7\n",
    "        #if s<31: \n",
    "        #    stroke=2\n",
    "        #    rigHeight=4.5\n",
    "        #    rigWidth=6\n",
    "        #if s<20: \n",
    "        #    stroke=1\n",
    "        #    rigHeight=4\n",
    "        #    rigWidth=6\n",
    "        #renderRig(diagScreen, cx,cy, s*rigWidth,s*rigHeight, (0,255,0), stroke)\n",
    "    #    if x+s*3>1280: maxX= 1280\n",
    "    #    else: maxX= x+s*3\n",
    "    #    v1= baseImage[y-s*2:y+s*2, x-s*3:maxX]\n",
    "    #    mask= cv2.inRange(v1, np.array([60,60,60]), np.array([255,255,255]) )\n",
    "    #    output= cv2.bitwise_and(v1, v1, mask= mask)\n",
    "    #    baseImage[y-s*2:y+s*2, x-s*3:maxX]= np.invert(np.dstack((mask,mask,mask)))\n",
    "    #    rx,ry,rs= x,y,s\n",
    "        prevCentroid= newCentroid\n",
    "    drawData(diagScreen)\n",
    "    lastDiagScreen= np.copy(diagScreen)\n",
    "\n",
    "def redrawBatch(baseImage): # redraw the last batch\n",
    "    global patchBatch, predictionBatch, diagScreen, lastDiagScreen\n",
    "#    global rx,ry,rs\n",
    "#    x,y,s= rx,ry,rs\n",
    "#    if x+s*3>1280: maxX= 1280\n",
    "#    else: maxX= x+s*3\n",
    "#    v1= baseImage[y-s*2:y+s*2, x-s*3:maxX]\n",
    "#    mask= cv2.inRange(v1, np.array([60,60,60]), np.array([255,255,255]) )\n",
    "#    output= cv2.bitwise_and(v1, v1, mask= mask)\n",
    "#    baseImage[y-s*2:y+s*2, x-s*3:maxX]= np.invert(np.dstack((mask,mask,mask)))\n",
    "    diagScreen= np.copy(lastDiagScreen)\n",
    "    patchBatch= [[], [], [], [], []]  # patch, x, y, s, prediction\n",
    "    predictionBatch= []\n",
    "\n",
    "def process(img):\n",
    "    global diagScreen, frameCount, imgGlobal\n",
    "    diagScreen= np.zeros_like(img).astype(np.uint8)  #*127\n",
    "    imgGlobal= img\n",
    "    #for i in range(15):\n",
    "    #if frameCount>=182 and frameCount<=189:\n",
    "    #if frameCount==0:                  # <-------------------------------------- FRAME\n",
    "    \n",
    "    if frameCount%2==0:\n",
    "        for i in range(0,15):\n",
    "            processStripC(i) # add patches to batch\n",
    "        processBatch(img)\n",
    "    else:\n",
    "        redrawBatch(img)\n",
    "\n",
    "    cv2.putText(img, str(frameCount), (1200,80), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255))\n",
    "    result= cv2.addWeighted(img, 1, diagScreen, 1, 0) # annotate the original\n",
    "    #result= diagScreen\n",
    "    frameCount+= 1\n",
    "    return result\n",
    "\n",
    "def procVideo(fileName):\n",
    "    global frameCount\n",
    "    frameCount= 0\n",
    "    clip= VideoFileClip(fileName)\n",
    "    imgName= fileName.split('/')[1]\n",
    "    project_video_output= 'output_images/'+imgName\n",
    "    print('Processing video...')\n",
    "    project_video_clip= clip.fl_image(process)\n",
    "    t=time.time()\n",
    "    project_video_clip.write_videofile(project_video_output, audio=False)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'seconds to complete. ', round(frameCount/(t2-t), 2), 'fps' )\n",
    "\n",
    "box_list= [[],[]]\n",
    "procVideo('video/project_video.mp4')  #UltraShort2  Black   project_video    Passing  challenge_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#_______________________________________________________________________________\n",
    "# Vehicle tracking class\n",
    "\n",
    "class Vehicle():\n",
    "    def __init__(self, color, left,top, width,height):\n",
    "        self.color= color\n",
    "        self.left= left\n",
    "        self.top= top\n",
    "        self.width= width\n",
    "        self.height= height\n",
    "        self.dx= 0  # relative speed (computed over frames)\n",
    "        self.dy= 0\n",
    "        self.refQ= NQ(25,(0,2))   #4 frames containing an array of refdata\n",
    "        self.centroidX= left+width/2\n",
    "        self.centroidY= top+height/2\n",
    "        \n",
    "    def updateCentroid(self):  #based on reference queue\n",
    "        #zip through the queue and update location of centroid\n",
    "        for i in range(len(self.refQ)):\n",
    "            pass #  data= self.refQ.peek()\n",
    "        lastX= self.centroidX\n",
    "        lastY= self.centroidY\n",
    "        self.centroidX= left+width/2\n",
    "        self.centroidY= top+height/2\n",
    "        #also update dx,dy\n",
    "        self.dx= self.centroidX-lastX\n",
    "        self.dy= self.centroidY-lastY\n",
    "    \n",
    "    #Resolve real-world patches to vehicle model\n",
    "    #Patch affinity rules:\n",
    "    #majority should be of one color (with darker lower on the image)\n",
    "    #if occluded, patches belong to top vehicle\n",
    "    def addReferenceData(self, dataList):  #dataList: [ [x,y,s,color], ... ]\n",
    "        #reject/accept data according to rule set\n",
    "        #modify dataList here\n",
    "        #majority should be within a 'normal' range wrt distance (either computed or ascertained over time)\n",
    "        # with consideration for dx,dy\n",
    "        #if black car, reject white cells\n",
    "        \n",
    "        #add resulting data to queue\n",
    "        self.refQ.put([ dataList ])\n",
    "    \n",
    "    def draw(self):\n",
    "        global diagScreen\n",
    "        pass # draw something to the screen\n",
    "        #first, try just the centroid, or the bounding ellipse\n",
    "        diagScreen[y:y+s-1, x:x+s-1, 2]= 255  # blue\n",
    "\n",
    "# Example init for 'Black' video\n",
    "carWhite= Vehicle('White', 862,415, 88,52)\n",
    "carBlack= Vehicle('Black', 1233,394, 576,343) #width,height is apx (offscreen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Debug cell\n",
    "\n",
    "master= np.zeros((60),dtype=np.int32)\n",
    "refM= master.reshape((4,15),order='F')  # does not work without fixing strides: ,order='F'\n",
    "refM.strides= (60,4)\n",
    "mask= np.zeros_like(refM) \n",
    "print(mask.shape,mask.ndim,mask.strides)\n",
    "cv2.ellipse(mask, ((7.5,1.5), (7,4),0), 1,-1)\n",
    "print(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img2= mpimg.imread('training/vehicles/JWD_vcells/442.PNG')\n",
    "img3= cv2.resize(img2, (4,4), interpolation=3)\n",
    "plt.imshow(img2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img= mpimg.imread('training/vehicles/JWD_vcells/442.PNG')\n",
    "def get_texture_features(img, quantization=8, radius=3):\n",
    "    lbp= np.ravel(local_binary_pattern(img, quantization, radius, method='uniform'))\n",
    "    return np.mean(lbp.reshape(-1, 4), axis=1)/(quantization+1)\n",
    "f = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "l= get_texture_features(f,4,3)\n",
    "print(l.shape)\n",
    "plt.plot(l)\n",
    "plt.title('Texture Features')\n",
    "plt.show()\n",
    "print(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[0 0 1 1 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [2 2 0 1 1 0]\n",
      " [0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[0,0,1,2,0,0],\n",
    "              [0,0,0,1,0,0],\n",
    "              [2,1,0,1,1,0],\n",
    "              [0,0,0,2,1,0]])\n",
    "labeled_array, num_features = label(a)\n",
    "print(num_features)\n",
    "print(labeled_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2 2 0 0 3]\n",
      " [2 2 2 0 0 0]\n",
      " [0 0 1 1 0 0]\n",
      " [0 0 1 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((6,6), dtype=np.int)\n",
    "a[2:4, 2:4] = 1\n",
    "a[4, 4] = 1\n",
    "a[:2, :3] = 2\n",
    "a[0, 5] = 3\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0]\n",
      " [1 1 0]\n",
      " [0 0 1]]\n",
      "[[2 2 2]\n",
      " [2 2 2]]\n",
      "[(slice(2, 5, None), slice(2, 5, None)), (slice(0, 2, None), slice(0, 3, None)), (slice(0, 1, None), slice(5, 6, None))]\n"
     ]
    }
   ],
   "source": [
    "from scipy import ndimage\n",
    "loc0= ndimage.find_objects(a)[0]\n",
    "print(a[loc0])\n",
    "loc1= ndimage.find_objects(a)[1]\n",
    "print(a[loc1])\n",
    "listObj= ndimage.find_objects(a)\n",
    "print(listObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 5 5\n",
      "(slice(0, 2, None), slice(0, 3, None))\n"
     ]
    }
   ],
   "source": [
    "# Accessing the properties of a slice\n",
    "print(loc0[0].start,loc0[1].start,   loc0[0].stop, loc0[1].stop)\n",
    "print(loc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(box_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
