{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding box (split version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.externals import joblib\n",
    "from keras.models import model_from_json\n",
    "from moviepy.editor import VideoFileClip\n",
    "from scipy.ndimage.measurements import label\n",
    "from skimage.feature import local_binary_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Metaparameters\n",
    "colorspace= 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient= 12\n",
    "pix_per_cell= 4\n",
    "cell_per_block= 1 #2\n",
    "hog_channel= 'ALL' # Can be 0, 1, 2, or \"ALL\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Customized re-implementation of HOG, based on OpenCV and scikit-image versions\n",
    "def histoGrad(img, binCount=12, cellSize=4):\n",
    "    gx,gy= cv2.Sobel(img,cv2.CV_32F,1,0), cv2.Sobel(img,cv2.CV_32F,0,1) # gradients in x and y direction\n",
    "    magnitude,angle= cv2.cartToPolar(gx,gy)\n",
    "    bins= np.int32(binCount*angle*(1-1e-7)/(2*np.pi)) #scale back angle to avoid overbinning\n",
    "    binCells,magCells= [],[]\n",
    "    cx= cy= cellSize\n",
    "    for i in range(0,int(img.shape[0]/cy)): # sort into bins\n",
    "        for j in range(0,int(img.shape[1]/cx)):\n",
    "            binCells.append(bins[i*cy:i*cy+cy, j*cx:j*cx+cx])\n",
    "            magCells.append(magnitude[i*cy:i*cy+cy, j*cx:j*cx+cx])\n",
    "    hist= np.hstack([np.bincount(i.ravel(), j.ravel(), binCount) for i,j in zip(binCells, magCells)])\n",
    "    return np.sqrt(hist/(hist.sum()+1e-8)) # apply Hellinger kernel (L1-normalize and sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to return HOG features\n",
    "def get_hog_features(img, orient, pix_per_cell):\n",
    "    features= histoGrad( img, orient, pix_per_cell )\n",
    "    return features.reshape((orient*pix_per_cell*pix_per_cell,))    #192\n",
    "\n",
    "# Function to return color histogram features  \n",
    "def get_color_hist(img, nbins=32, bins_range=(0, 32)):\n",
    "    return np.histogram(img, bins=nbins, range=bins_range)\n",
    "\n",
    "# Function to return spatial binning of color features\n",
    "def get_spatial(img, size=(8, 8)):\n",
    "    return np.ravel(cv2.resize(img, size, interpolation=3) )/255.\n",
    "\n",
    "# Function to return texture features\n",
    "def get_texture_features(img, quantization=8, radius=3):\n",
    "    lbp= np.ravel(local_binary_pattern(img, quantization, radius, method='uniform'))\n",
    "    return np.mean(lbp.reshape(-1, 4), axis=1) # average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dropout_1 (Dropout)              (None, 624)           0           dropout_input_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 255)           159375      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 255)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 127)           32512       dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 127)           0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             128         dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 192,015\n",
      "Trainable params: 192,015\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load binary classifier model\n",
    "\n",
    "model= None\n",
    "with open('model.json', 'r') as jfile:\n",
    "    model = model_from_json(json.loads(jfile.read()))\n",
    "model.compile(\"adam\", \"binary_crossentropy\")\n",
    "#weights_file = args.model.replace('json', 'h5')\n",
    "model.load_weights('model.h5')\n",
    "model.summary()\n",
    "X_scaler= joblib.load('scalar.dat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cell' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1cc7813a2631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcell\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cell' is not defined"
     ]
    }
   ],
   "source": [
    "cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getFeaturesFromColorPatch(patch, orient=12, pix_per_cell=4, cell_per_block=1):\n",
    "    global firstTime\n",
    "    hog_features = []\n",
    "    feature_patch= cv2.cvtColor(patch, cv2.COLOR_RGB2YCrCb)\n",
    "    for channel in range(feature_patch.shape[2]):\n",
    "        #hog_features.append(get_hog_features(feature_patch[:,:,channel], \n",
    "        #                    orient, pix_per_cell, cell_per_block, \n",
    "        #                    vis=False, feature_vec=True))\n",
    "        \n",
    "        # Compute gradient AND small color patch (note: color scaling)\n",
    "        hog_features.append(np.concatenate((get_hog_features(feature_patch[:,:,channel], orient, pix_per_cell), \n",
    "            np.ravel(cv2.resize(feature_patch[:,:,channel], (4,4), interpolation=3) )/255. ))) #CV_INTER_AREA\n",
    "        #hog_features.append(np.concatenate((get_hog_features(feature_patch[:,:,channel], \n",
    "        #                    orient, pix_per_cell, cell_per_block, \n",
    "        #                    vis=False, feature_vec=True), get_lpb_features(feature_patch[:,:,channel]))))\n",
    "        #hog_features.append(np.zeros(432))   # <--- short out\n",
    "    return np.ravel(hog_features)\n",
    "\n",
    "def scalePatch(patch):\n",
    "    global X_scaler # use previously defined scaler\n",
    "    X= patch.astype(np.float64).reshape((1, -1))                       \n",
    "    # Apply the scaler to X\n",
    "    return np.array(X_scaler.transform(X))\n",
    "\n",
    "#_______________________________________________________________________________\n",
    "# Circular queuing class made with Numpy\n",
    "\n",
    "class NQ():\n",
    "    def __init__(self,qlen,shape,dtype=np.uint8):\n",
    "        self.data= np.empty(shape=shape,dtype=dtype)\n",
    "        self.qlen= qlen\n",
    "    def put(self,a):\n",
    "        self.data= np.append(self.data,a, axis=0)\n",
    "        if len(self.data)>self.qlen: self.drop()\n",
    "    def peek(self):\n",
    "        return self.data[len(self.data)-1]\n",
    "    def drop(self):\n",
    "        self.data= np.delete(self.data, 0, axis=0)\n",
    "    def getAvg(self):\n",
    "        newList= np.copy(self.data[0])\n",
    "        newList[:]= np.sum(self.data[:], axis=0)/float(len(self.data))\n",
    "        return newList\n",
    "\n",
    "def renderRig(img, cx,cy, width,height, clr, strokeWeight):\n",
    "    pts=np.array([[0.75,0.1275], [0.4866666667,0.085], [0.286666667,0.1025], [0,0.3875], [0,0.645], [0.47,0.9275], [1,0.8825], [1,0.38]])\n",
    "    pts[:,0]= pts[:,0]*width  + cx-width //2\n",
    "    pts[:,1]= pts[:,1]*height + cy-height//2\n",
    "    cv2.polylines(img, [pts.astype(np.int32)], True, clr, strokeWeight)\n",
    "\n",
    "x= np.linspace(0, 14, 15)\n",
    "y=np.array([766,780,796,814,834,857,882,910,941,976,1014,1057,1103,1155,1214])\n",
    "coefs1= np.polyfit(x,y,3)\n",
    "y=np.array([411,410,410,409,409,408,407,406,406,405,403,402,401,398,397])\n",
    "coefs2= np.polyfit(x,y,2)\n",
    "y=np.array([14,16,18,20,23,25,28,31,35,38,43,46,52,59,66])\n",
    "coefs3= np.polyfit(x,y,2)\n",
    "def transformIdxToScreen(idxFloat):\n",
    "    global coefs1, coefs2, coefs3\n",
    "    f1= np.poly1d(coefs1)\n",
    "    f2= np.poly1d(coefs2)\n",
    "    f3= np.poly1d(coefs3)\n",
    "    return f1(idxFloat), f2(idxFloat), f3(idxFloat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "    return heatmap\n",
    "    \n",
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    return heatmap\n",
    "\n",
    "def draw_labeled_bboxes(img, labels):\n",
    "    global frameCount\n",
    "    # Iterate through all detected cars\n",
    "#    if labels[1]>2: print(frameCount,labels[1])\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero= (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy= np.array(nonzero[0])\n",
    "        nonzerox= np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox= ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        \n",
    "        # Draw the box on the image\n",
    "        # RULE: if bbox is over 2.8:1 aspect ratio, split in two\n",
    "        bWidth= bbox[1][0]-bbox[0][0]    #x1-x0\n",
    "        bHeight= bbox[1][1]-bbox[0][1]   #y1-y0\n",
    "        #if frameCount==240: print(bWidth,bHeight,bWidth/bHeight,((bbox[0][0]+bWidth//2,bbox[0][1]), (bbox[0][0]+bWidth//2,bbox[1][1])))\n",
    "        if bHeight>100 and bWidth/bHeight>2.68:\n",
    "            mid= ((bbox[0][0]+bWidth//2,bbox[0][1]), (bbox[0][0]+bWidth//2,bbox[1][1]))\n",
    "            cv2.rectangle(img, bbox[0], mid[1], (0,150,255), 5)\n",
    "            cv2.rectangle(img, mid[0], bbox[1], (0,150,255), 5)\n",
    "        else:\n",
    "            # RULE: do not render boxes of height less than min cell size\n",
    "            if bHeight>=14 and bWidth>=14:\n",
    "                cv2.rectangle(img, bbox[0], bbox[1], (0,150,255), 5)\n",
    "    return img\n",
    "\n",
    "box_list= []\n",
    "#box_list= NQ(6, (0,2), np.int)\n",
    "\n",
    "def addBoxToList(x,y,s): #add a screen box for each of the positive cells\n",
    "    global box_list\n",
    "    box_list.append( ((x, y), (x+s+5, y+s+5)) ) # make the cells slightly larger so they overlap\n",
    "\n",
    "def drawData(img):\n",
    "    global box_list, frameCount\n",
    "    heat= np.zeros_like(img[:,:,0]).astype(np.float)\n",
    "    # Add heat to each box in box list\n",
    "    heat= add_heat(heat,box_list)\n",
    "\n",
    "    # Apply threshold to help remove false positives\n",
    "    heat= apply_threshold(heat,1)\n",
    "\n",
    "    # Find final boxes from heatmap using label function\n",
    "    labels= label(heat)  #label(heat)\n",
    "    #if frameCount==246:\n",
    "    #    plt.figure(figsize=(16,9))\n",
    "    #    plt.imshow(np.clip(heat, 0, 255), cmap='hot')\n",
    "    #    plt.show()\n",
    "    draw_labeled_bboxes(img, labels)\n",
    "    if frameCount==246:\n",
    "        heat2= np.zeros_like(img)\n",
    "        heat2[:,:,0]= np.clip((255./np.max(np.clip(heat, 0, 255))) * heat,  0, 255).astype(np.uint8)\n",
    "        mpimg.imsave('output_images/cells/frame246heat.PNG',heat2)\n",
    "        draw_labeled_bboxes(heat2, labels)\n",
    "        mpimg.imsave('output_images/cells/frame246heatboxes.PNG',heat2)\n",
    "        \n",
    "    \n",
    "    # Trim the box list\n",
    "    boxQLEN= 50\n",
    "    if len(box_list)>boxQLEN: # keep the last n boxes\n",
    "        top= len(box_list)-boxQLEN\n",
    "        for i in range(0,top): # circular queue compatible with code from lecture notes \n",
    "            del box_list[0]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video...\n",
      "[MoviePy] >>>> Building video output_images/Black.mp4\n",
      "[MoviePy] Writing video output_images/Black.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 251/251 [00:24<00:00, 10.43it/s]  1%|▉                                                                                                                           | 2/251 [00:00<00:24, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_images/Black.mp4 \n",
      "\n",
      "25.59 seconds to complete @ 9.85 fps\n"
     ]
    }
   ],
   "source": [
    "# Batch version\n",
    "def resizePatch(img):\n",
    "    if len(img[0]) != 16:\n",
    "        if len(img[0])>16:\n",
    "            patch= cv2.resize(img, (16,16), interpolation=3) #CV_INTER_AREA\n",
    "        else:\n",
    "            patch= cv2.resize(img, (16,16), interpolation=2) #CV_INTER_CUBIC\n",
    "    else:\n",
    "        patch= img\n",
    "    return patch\n",
    "\n",
    "def preparePatch(p):\n",
    "    global frameCount\n",
    "    patch= resizePatch(p)\n",
    "    fpatch= getFeaturesFromColorPatch(patch)\n",
    "    spatch= scalePatch(fpatch)\n",
    "    return spatch\n",
    "\n",
    "diagScreen= None\n",
    "imgGlobal= None\n",
    "patchBatch= [[], [], [], [], []]  # patch, x, y, s, prediction\n",
    "predictionBatch= []\n",
    "\n",
    "def addPatch(p,x,y,s):\n",
    "    global patchBatch\n",
    "    patchBatch[0].append(p)\n",
    "    patchBatch[1].append(x)\n",
    "    patchBatch[2].append(y)\n",
    "    patchBatch[3].append(s)\n",
    "\n",
    "def processStripB(x,y,s):\n",
    "    global frameCount, imgGlobal\n",
    "    for i in range(4):\n",
    "        addPatch(imgGlobal[y:y+s, x:x+s, :],x,y,s)\n",
    "        y= y+s\n",
    "\n",
    "def processStripC(i):\n",
    "    global frameCount, imgGlobal\n",
    "    x,y,s= transformIdxToScreen(i)\n",
    "    for i in range(4):\n",
    "        addPatch(imgGlobal[y:y+s, x:x+s, :],x,y,s)\n",
    "        y= y+s\n",
    "\n",
    "# Predominant color assessment\n",
    "def dominantWhite(img): \n",
    "    return (img>90).sum() > (img<=90).sum()\n",
    "\n",
    "def locateCentroid(twoDimArray, prevCenterTuple):\n",
    "    global frameCount\n",
    "    mask= np.zeros((4,15),dtype=np.int32)\n",
    "    #mask= np.zeros_like(twoDimArray)               # <-----------------------------------------------------\n",
    "\n",
    "    cv2.ellipse(mask, (prevCenterTuple, (7,4),0), 1,-1)\n",
    "    #mask= np.array(mask, dtype=np.uint8)\n",
    "    masked= cv2.bitwise_and(mask, twoDimArray)\n",
    "    m= cv2.moments(np.array(masked),True)\n",
    "    return m['m10']/m['m00'], m['m01']/m['m00'] #return centroid float: x,y or col,row\n",
    "\n",
    "prevCentroid= (17.,2.)\n",
    "centroidQ= NQ(25, (0,2), np.int)\n",
    "for i in range(25): # bias average to offscreen \n",
    "    centroidQ.put([ np.array([1400,529]) ])\n",
    "lastDiagScreen= None\n",
    "rx= 0\n",
    "ry= 0\n",
    "rs= 0\n",
    "\n",
    "def processBatch(baseImage): # send the batch to the GPU for predictions\n",
    "    global frameCount, patchBatch, predictionBatch, diagScreen, prevCentroid, lastDiagScreen\n",
    "    global rx,ry,rs\n",
    "    prepBatch= []\n",
    "    for i in range( len(patchBatch[0]) ):\n",
    "        prepBatch.append( preparePatch(patchBatch[0][i]) )\n",
    "    predictionResults= model.predict( np.array(prepBatch).reshape((60,-1)) )\n",
    "    patchBatch[4]= np.array(predictionResults>0.9,dtype=np.uint8)\n",
    "    whiteCarFoundMaybe= False\n",
    "    for i in range( len(patchBatch[4]) ):\n",
    "        x= patchBatch[1][i]\n",
    "        y= patchBatch[2][i]\n",
    "        s= patchBatch[3][i]\n",
    "        if patchBatch[4][i]:\n",
    "            addBoxToList(x,y,s)\n",
    "            \n",
    "#            if dominantWhite(patchBatch[0][i]):\n",
    "#                addBoxToList(x,y,s) #again\n",
    "#                diagScreen[y:y+s-1, x:x+s-1, 0]= 63 # red\n",
    "#                diagScreen[y:y+s-1, x:x+s-1, 1]= 31 # green\n",
    "#                if whiteCarFoundMaybe==False:\n",
    "#                    whiteCarFoundMaybe= True\n",
    "#                    whiteCarLoc= i\n",
    "#            else:\n",
    "#                diagScreen[y:y+s-1, x:x+s-1, 1]= 63 # green\n",
    "#        else:\n",
    "#            diagScreen[y:y+s-1, x:x+s-1, 0]= 63   #157  Red\n",
    "        # More training data\n",
    "        #if predictionResults[i]>0.90 and predictionResults[i]<0.92:\n",
    "        #cellViz= np.append(prepBatch[i],[0])\n",
    "        #if predictionResults[i]>0.99: print(prepBatch[i])\n",
    "        #cellViz= np.reshape(cellViz,(25,25))\n",
    "        \n",
    "        #cellViz= np.zeros((16,16,3),dtype=np.uint8)\n",
    "        #ones= np.ones((16,16,1),dtype=np.uint8)\n",
    "        #cellViz[:,:,1]= (predictionResults[i]*127).astype(np.uint8)\n",
    "        #cellViz[:,:,0]= ((1-predictionResults[i])*127).astype(np.uint8)\n",
    "        #mpimg.imsave('output_images/cells/'+str(frameCount)+'-'+str(int(x))+'-'+str(int(y))+'.PNG',cellViz)\n",
    "        \n",
    "        #mpimg.imsave('output_images/cells/'+str(frameCount)+'-'+str(int(x))+'-'+str(int(y))+'.PNG',resizePatch(patchBatch[0][i]))\n",
    "    #Search the neighbors of the white car in all directions\n",
    "#    if whiteCarFoundMaybe:\n",
    "#        refM= np.array(patchBatch[4].reshape((4,15),order='F'), dtype=np.uint8) # reference data matrix\n",
    "#        whiteCarLeft= 0 # assume maxes\n",
    "#        whiteCarRight= 14\n",
    "#        whiteCarTop= 0\n",
    "#        whiteCarBottom= 3\n",
    "#        if whiteCarLoc>4 and whiteCarLoc<56 and predictionBatch[i+4]>0.5 and predictionBatch[i-4]>0.5:\n",
    "\n",
    "        \n",
    "    if frameCount>=6000:  #  678 is when Black car arrives in project_video\n",
    "        for i in range(len(patchBatch[4])):\n",
    "            if dominantWhite(patchBatch[0][i]): patchBatch[4][i]=0 #mask white car for test purposes\n",
    "        refM= np.array(patchBatch[4].reshape((4,15),order='F'), dtype=np.int32) # reference data matrix\n",
    "        #refM.strides= (60,4)\n",
    "\n",
    "        newCentroid= locateCentroid(refM, prevCentroid)\n",
    "\n",
    "        newCentroid= (newCentroid[0], 1.5) #lock to middle of track\n",
    "\n",
    "        refX= np.array(patchBatch[1]).reshape((4,15),order='F')\n",
    "        refY= np.array(patchBatch[2]).reshape((4,15),order='F')\n",
    "        refS= np.array(patchBatch[3]).reshape((4,15),order='F')\n",
    "        x= refX[int(newCentroid[1]),round(newCentroid[0])]\n",
    "        y= refY[int(newCentroid[1]),int(newCentroid[0])]\n",
    "        #s= refS[int(newCentroid[1]),int(newCentroid[0])]\n",
    "        _,_,s= transformIdxToScreen(newCentroid[0])\n",
    "        x+= int( (newCentroid[0] - int(newCentroid[0]) ) * s) # add the fractional portion      s/2 +\n",
    "        y+= int(s/2 + (newCentroid[1] - int(newCentroid[1]) ) * s)\n",
    "        #diagScreen[y:y+10, x:x+10, 2]= 255  # small blue 10x10 square\n",
    "        centroidQ.put([ np.array([x,y]) ])\n",
    "\n",
    "        cv2.ellipse(diagScreen, (centroidQ.getAvg(), (s*6,s*4),0), (255,255,0), 2) # ellipse\n",
    "        #cx,cy= centroidQ.getAvg()\n",
    "        #stroke=3\n",
    "        #rigHeight=5\n",
    "        #rigWidth=7\n",
    "        #if s<31: \n",
    "        #    stroke=2\n",
    "        #    rigHeight=4.5\n",
    "        #    rigWidth=6\n",
    "        #if s<20: \n",
    "        #    stroke=1\n",
    "        #    rigHeight=4\n",
    "        #    rigWidth=6\n",
    "        #renderRig(diagScreen, cx,cy, s*rigWidth,s*rigHeight, (0,255,0), stroke)\n",
    "    #    if x+s*3>1280: maxX= 1280\n",
    "    #    else: maxX= x+s*3\n",
    "    #    v1= baseImage[y-s*2:y+s*2, x-s*3:maxX]\n",
    "    #    mask= cv2.inRange(v1, np.array([60,60,60]), np.array([255,255,255]) )\n",
    "    #    output= cv2.bitwise_and(v1, v1, mask= mask)\n",
    "    #    baseImage[y-s*2:y+s*2, x-s*3:maxX]= np.invert(np.dstack((mask,mask,mask)))\n",
    "    #    rx,ry,rs= x,y,s\n",
    "        prevCentroid= newCentroid\n",
    "    drawData(diagScreen)\n",
    "    lastDiagScreen= np.copy(diagScreen)\n",
    "\n",
    "def redrawBatch(baseImage): # redraw the last batch\n",
    "    global patchBatch, predictionBatch, diagScreen, lastDiagScreen\n",
    "#    global rx,ry,rs\n",
    "#    x,y,s= rx,ry,rs\n",
    "#    if x+s*3>1280: maxX= 1280\n",
    "#    else: maxX= x+s*3\n",
    "#    v1= baseImage[y-s*2:y+s*2, x-s*3:maxX]\n",
    "#    mask= cv2.inRange(v1, np.array([60,60,60]), np.array([255,255,255]) )\n",
    "#    output= cv2.bitwise_and(v1, v1, mask= mask)\n",
    "#    baseImage[y-s*2:y+s*2, x-s*3:maxX]= np.invert(np.dstack((mask,mask,mask)))\n",
    "    diagScreen= np.copy(lastDiagScreen)\n",
    "    patchBatch= [[], [], [], [], []]  # patch, x, y, s, prediction\n",
    "    predictionBatch= []\n",
    "\n",
    "def process(img):\n",
    "    global diagScreen, frameCount, imgGlobal\n",
    "    diagScreen= np.zeros_like(img).astype(np.uint8)  #*127\n",
    "    imgGlobal= img\n",
    "    #for i in range(15):\n",
    "    #if frameCount>=182 and frameCount<=189:\n",
    "    #if frameCount==246:                  # <-------------------------------------- FRAME\n",
    "    \n",
    "    if frameCount%2==0:\n",
    "        for i in range(0,15):\n",
    "            processStripC(i) # add patches to batch\n",
    "        processBatch(img)\n",
    "    else:\n",
    "        redrawBatch(img)\n",
    "\n",
    "    cv2.putText(img, str(frameCount), (1200,80), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255))\n",
    "    result= cv2.addWeighted(img, 1, diagScreen, 1, 0) # annotate the original\n",
    "    #result= diagScreen\n",
    "    frameCount+= 1\n",
    "    return result\n",
    "\n",
    "def procVideo(fileName):\n",
    "    global frameCount\n",
    "    frameCount= 0\n",
    "    clip= VideoFileClip(fileName)\n",
    "    imgName= fileName.split('/')[1]\n",
    "    project_video_output= 'output_images/'+imgName\n",
    "    print('Processing video...')\n",
    "    project_video_clip= clip.fl_image(process)\n",
    "    t=time.time()\n",
    "    project_video_clip.write_videofile(project_video_output, audio=False)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'seconds to complete @', round(frameCount/(t2-t), 2), 'fps' )\n",
    "\n",
    "box_list= []\n",
    "procVideo('video/Black.mp4')  #UltraShort2  Black   project_video    Passing  challenge_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#_______________________________________________________________________________\n",
    "# Vehicle tracking class\n",
    "\n",
    "class Vehicle():\n",
    "    def __init__(self, color, left,top, width,height):\n",
    "        self.color= color\n",
    "        self.left= left\n",
    "        self.top= top\n",
    "        self.width= width\n",
    "        self.height= height\n",
    "        self.dx= 0  # relative speed (computed over frames)\n",
    "        self.dy= 0\n",
    "        self.refQ= NQ(25,(0,2))   #4 frames containing an array of refdata\n",
    "        self.centroidX= left+width/2\n",
    "        self.centroidY= top+height/2\n",
    "        \n",
    "    def updateCentroid(self):  #based on reference queue\n",
    "        #zip through the queue and update location of centroid\n",
    "        for i in range(len(self.refQ)):\n",
    "            pass #  data= self.refQ.peek()\n",
    "        lastX= self.centroidX\n",
    "        lastY= self.centroidY\n",
    "        self.centroidX= left+width/2\n",
    "        self.centroidY= top+height/2\n",
    "        #also update dx,dy\n",
    "        self.dx= self.centroidX-lastX\n",
    "        self.dy= self.centroidY-lastY\n",
    "    \n",
    "    #Resolve real-world patches to vehicle model\n",
    "    #Patch affinity rules:\n",
    "    #majority should be of one color (with darker lower on the image)\n",
    "    #if occluded, patches belong to top vehicle\n",
    "    def addReferenceData(self, dataList):  #dataList: [ [x,y,s,color], ... ]\n",
    "        #reject/accept data according to rule set\n",
    "        #modify dataList here\n",
    "        #majority should be within a 'normal' range wrt distance (either computed or ascertained over time)\n",
    "        # with consideration for dx,dy\n",
    "        #if black car, reject white cells\n",
    "        \n",
    "        #add resulting data to queue\n",
    "        self.refQ.put([ dataList ])\n",
    "    \n",
    "    def draw(self):\n",
    "        global diagScreen\n",
    "        pass # draw something to the screen\n",
    "        #first, try just the centroid, or the bounding ellipse\n",
    "        diagScreen[y:y+s-1, x:x+s-1, 2]= 255  # blue\n",
    "\n",
    "# Example init for 'Black' video\n",
    "carWhite= Vehicle('White', 862,415, 88,52)\n",
    "carBlack= Vehicle('Black', 1233,394, 576,343) #width,height is apx (offscreen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Debug cell\n",
    "\n",
    "master= np.zeros((60),dtype=np.int32)\n",
    "refM= master.reshape((4,15),order='F')  # does not work without fixing strides: ,order='F'\n",
    "refM.strides= (60,4)\n",
    "mask= np.zeros_like(refM) \n",
    "print(mask.shape,mask.ndim,mask.strides)\n",
    "cv2.ellipse(mask, ((7.5,1.5), (7,4),0), 1,-1)\n",
    "print(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img2= mpimg.imread('training/vehicles/JWD_vcells/442.PNG')\n",
    "img3= cv2.resize(img2, (4,4), interpolation=3)\n",
    "plt.imshow(img2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img= mpimg.imread('training/vehicles/JWD_vcells/442.PNG')\n",
    "def get_texture_features(img, quantization=8, radius=3):\n",
    "    lbp= np.ravel(local_binary_pattern(img, quantization, radius, method='uniform'))\n",
    "    return np.mean(lbp.reshape(-1, 4), axis=1)/(quantization+1)\n",
    "f = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "l= get_texture_features(f,4,3)\n",
    "print(l.shape)\n",
    "plt.plot(l)\n",
    "plt.title('Texture Features')\n",
    "plt.show()\n",
    "print(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[[0 0 1 1 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [2 2 0 1 1 0]\n",
      " [0 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[0,0,1,2,0,0],\n",
    "              [0,0,0,1,0,0],\n",
    "              [2,1,0,1,1,0],\n",
    "              [0,0,0,2,1,0]])\n",
    "labeled_array, num_features = label(a)\n",
    "print(num_features)\n",
    "print(labeled_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2 2 0 0 3]\n",
      " [2 2 2 0 0 0]\n",
      " [0 0 1 1 0 0]\n",
      " [0 0 1 1 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((6,6), dtype=np.int)\n",
    "a[2:4, 2:4] = 1\n",
    "a[4, 4] = 1\n",
    "a[:2, :3] = 2\n",
    "a[0, 5] = 3\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0]\n",
      " [1 1 0]\n",
      " [0 0 1]]\n",
      "[[2 2 2]\n",
      " [2 2 2]]\n",
      "[(slice(2, 5, None), slice(2, 5, None)), (slice(0, 2, None), slice(0, 3, None)), (slice(0, 1, None), slice(5, 6, None))]\n"
     ]
    }
   ],
   "source": [
    "from scipy import ndimage\n",
    "loc0= ndimage.find_objects(a)[0]\n",
    "print(a[loc0])\n",
    "loc1= ndimage.find_objects(a)[1]\n",
    "print(a[loc1])\n",
    "listObj= ndimage.find_objects(a)\n",
    "print(listObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 5 5\n",
      "(slice(0, 2, None), slice(0, 3, None))\n"
     ]
    }
   ],
   "source": [
    "# Accessing the properties of a slice\n",
    "print(loc0[0].start,loc0[1].start,   loc0[0].stop, loc0[1].stop)\n",
    "print(loc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images complete\n"
     ]
    }
   ],
   "source": [
    "def get_hog_features(img, orient, pix_per_cell,fname):\n",
    "    features= histoGrad( img, orient, pix_per_cell )\n",
    "    if (len(features)!=192): print(fname,len(features))\n",
    "    return features.reshape((orient*pix_per_cell*pix_per_cell,))    #192\n",
    "\n",
    "def getFeatures(feature_image,fname):\n",
    "    hog_features= []\n",
    "    for channel in range(feature_image.shape[2]):\n",
    "        # Train on gradient AND resized color patch\n",
    "        hog_features.append(np.concatenate((get_hog_features(feature_image[:,:,channel], 12, 4,fname), \n",
    "            np.ravel(cv2.resize(feature_image[:,:,channel], (4,4), interpolation=3) ) ))) #CV_INTER_AREA\n",
    "    return np.ravel(hog_features)        \n",
    "\n",
    "\n",
    "def getFeaturesMakeCells(feature_image,fname):\n",
    "    hog_features= []\n",
    "    quad= []\n",
    "    quad.append(feature_image[:, 0:16,:])\n",
    "    quad.append(feature_image[:,16:32,:])\n",
    "    quad.append(feature_image[:,32:48,:])\n",
    "    quad.append(feature_image[:,48:64,:])\n",
    "    for i in range(4):\n",
    "        #next, slice the quad into cells(16x16) by row\n",
    "        square= []\n",
    "        square.append(quad[i][ 0:16,:,:])\n",
    "        square.append(quad[i][16:32,:,:])\n",
    "        square.append(quad[i][32:48,:,:])\n",
    "        square.append(quad[i][48:64,:,:])\n",
    "        for j in range(4):\n",
    "            feature_image= square[j]\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.append(np.concatenate((get_hog_features(feature_image[:,:,channel], 12, 4,fname), \n",
    "                np.ravel(cv2.resize(feature_image[:,:,channel], (4,4), interpolation=3) ) ))) #CV_INTER_AREA\n",
    "    return np.ravel(hog_features)        \n",
    "\n",
    "#allImg= []\n",
    "#vimages = glob.glob('training/non-vehicles/JWD_ncells/*.png')\n",
    "#for image in vimages:\n",
    "#    allImg.append(image)\n",
    "#vimages = glob.glob('training/non-vehicles/JWD_ncells_ch/*.png')\n",
    "#for image in vimages:\n",
    "#    allImg.append(image)\n",
    "#vimages = glob.glob('training/vehicles/JWD_vcells/*.png')\n",
    "#for image in vimages:\n",
    "#    allImg.append(image)\n",
    "#vimages = glob.glob('training/vehicles/JWD_vcells_ch/*.png')\n",
    "#for image in vimages:\n",
    "#    allImg.append(image)\n",
    "#\n",
    "#first= True\n",
    "#for fname in allImg:\n",
    "#    image= mpimg.imread(fname)\n",
    "#    yimage= cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "#    fimage= getFeatures(yimage,fname)\n",
    "#    if first and (np.any(fimage>1) or np.any(fimage<0)):\n",
    "#        print(fname)\n",
    "#        first= False\n",
    "#        break\n",
    "#\n",
    "#print('Cells complete')\n",
    "\n",
    "allImg= []\n",
    "vimages = glob.glob('training/non-vehicles/Extras/*.png')\n",
    "for image in vimages:\n",
    "    allImg.append(image)\n",
    "\n",
    "first= True\n",
    "for fname in allImg:\n",
    "    image= mpimg.imread(fname)\n",
    "    yimage= cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "    fimage= getFeaturesMakeCells(yimage,fname)\n",
    "    if first and (np.any(fimage>1) or np.any(fimage<0)):\n",
    "        print(fname)\n",
    "        first= False\n",
    "        break\n",
    "\n",
    "print('Images complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "y=np.array([0,0,0,2,0])\n",
    "if np.any(x>1): \n",
    "    print(x>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.349298   -2.47323748  0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [-3.46939396  0.3934696   0.14142136]\n",
      " [-1.77007604 -2.95102199  0.14142136]\n",
      " [ 0.349298   -0.08431491  0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [ 0.349298   -2.47323748  0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [-3.46939396  0.3934696  -7.07106781]\n",
      " [-1.77007604 -2.95102199  0.14142136]\n",
      " [ 0.349298   -0.08431491  0.14142136]\n",
      " [ 0.349298   -2.47323748  0.14142136]\n",
      " [ 0.349298    0.3934696   0.14142136]\n",
      " [-3.46939396  0.3934696   0.14142136]\n",
      " [-1.77007604 -2.95102199  0.14142136]\n",
      " [ 0.349298   -0.08431491  0.14142136]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create an array stack of feature vectors\n",
    "X= np.vstack(([[1,.4,1],[1,1,1],[.8,1,1],[.889,.3,1],[1,.9,1]], \n",
    "             [[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1]], \n",
    "             [[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1]], \n",
    "             [[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1]], \n",
    "             [[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1]], \n",
    "             [[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1]], \n",
    "             [[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1]], \n",
    "             [[1,.4,1],[1,1,1],[.8,1,0],[.889,.3,1],[1,.9,1]], \n",
    "             [[1,.4,1],[1,1,1],[.8,1,1],[.889,.3,1],[1,.9,1]]) ) #.astype(np.float64)                        \n",
    "# Fit a per-column scaler\n",
    "X_scaler= StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X= X_scaler.transform(X)\n",
    "print(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.35355339  0.04867924  1.62018517  1.41117638  1.11803399  1.36301867\n",
      "   0.84002688  1.41421356  1.41117638  1.24844311 -0.12555049  1.11803399\n",
      "   1.29907569  1.06155157]\n",
      " [-0.35355339  1.36301867  1.62018517  1.41117638  1.11803399  1.36301867\n",
      "   1.27204071  1.41421356  1.41117638  1.49241703  1.45638569  1.11803399\n",
      "   1.29907569  1.27868712]\n",
      " [ 2.82842712 -0.82754705 -0.69436507 -0.74146555 -0.89442719 -0.82754705\n",
      "  -0.88802842 -0.70710678 -0.74146555 -0.7055462  -0.80352314 -0.89442719\n",
      "  -0.86601037 -0.89266837]\n",
      " [-0.35355339 -0.82754705 -0.69436507 -0.74146555 -0.89442719 -0.82754705\n",
      "  -0.88802842 -0.70710678 -0.74146555 -0.7055462  -0.80352314 -0.89442719\n",
      "  -0.86601037 -0.89266837]\n",
      " [-0.35355339 -0.82754705 -0.69436507 -0.74146555 -0.89442719 -0.82754705\n",
      "  -0.88802842 -0.70710678 -0.74146555 -0.7055462  -0.80352314 -0.89442719\n",
      "  -0.86601037 -0.89266837]\n",
      " [-0.35355339 -0.82754705 -0.69436507 -0.74146555 -0.89442719 -0.82754705\n",
      "  -0.88802842 -0.70710678 -0.74146555 -0.7055462  -0.80352314 -0.89442719\n",
      "  -0.86601037 -0.89266837]\n",
      " [-0.35355339 -0.82754705 -0.69436507 -0.74146555 -0.89442719 -0.82754705\n",
      "  -0.88802842 -0.70710678 -0.74146555 -0.7055462  -0.80352314 -0.89442719\n",
      "  -0.86601037 -0.89266837]\n",
      " [-0.35355339  1.36301867  0.9258201   1.41117638  1.11803399  1.36301867\n",
      "   1.05603379  1.41421356  1.41117638  1.49241703  1.45638569  1.11803399\n",
      "   0.43282476  1.06155157]\n",
      " [-0.35355339  1.36301867 -0.69436507 -0.52620136  1.11803399  0.04867924\n",
      "   1.27204071 -0.70710678 -0.52620136 -0.7055462   1.2303948   1.11803399\n",
      "   1.29907569  1.06155157]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "X_scaled = preprocessing.scale(X)\n",
    "print(X_scaled)\n",
    "X_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[[ 0.59904584 -1.38961667  0.2236068 ]\n",
    " [ 0.59904584  0.69480833  0.2236068 ]\n",
    " [-2.09762353  0.69480833  0.2236068 ]\n",
    " [-0.89760566 -1.73702083  0.2236068 ]\n",
    " [ 0.59904584  0.34740417  0.2236068 ]\n",
    " [ 0.59904584  0.69480833  0.2236068 ]\n",
    " [ 0.59904584  0.69480833  0.2236068 ]\n",
    " [ 0.59904584  0.69480833  0.2236068 ]\n",
    " [ 0.59904584  0.69480833  0.2236068 ]\n",
    " [ 0.59904584  0.69480833  0.2236068 ]\n",
    " [ 0.59904584  0.69480833  0.2236068 ]\n",
    " [ 0.59904584 -1.38961667  0.2236068 ]\n",
    " [ 0.59904584  0.69480833  0.2236068 ]\n",
    " [-2.09762353  0.69480833 -4.47213595]\n",
    " [-0.89760566 -1.73702083  0.2236068 ]\n",
    " [ 0.59904584  0.34740417  0.2236068 ]\n",
    " [ 0.59904584 -1.38961667  0.2236068 ]\n",
    " [ 0.59904584  0.69480833  0.2236068 ]\n",
    " [-2.09762353  0.69480833  0.2236068 ]\n",
    " [-0.89760566 -1.73702083  0.2236068 ]\n",
    " [ 0.59904584  0.34740417  0.2236068 ]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1]\n",
      "  [2]]\n",
      "\n",
      " [[3]\n",
      "  [4]]]\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3,4]\n",
    "a= np.reshape(a,(2,2,1))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(np.ones((16,16),np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "[ 12   3  18 255]\n"
     ]
    }
   ],
   "source": [
    "heat = np.array([4,1,6,85])\n",
    "print(np.max(heat))\n",
    "print(np.clip((255./np.max(np.clip(heat, 0, 255))) * heat,  0, 255).astype(np.uint8) )\n",
    "#print((255./np.max(np.clip(heat, 0, 255))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
